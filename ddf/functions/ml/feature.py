#!/usr/bin/python
# -*- coding: utf-8 -*-

__author__ = "Lucas Miguel S Ponce"
__email__ = "lucasmsp@gmail.com"

from pycompss.api.task import task
from pycompss.functions.reduce import merge_reduce
from pycompss.api.api import compss_wait_on
from ddf.ddf import COMPSsContext, DDF, ModelDDS
import numpy as np
import pandas as pd
import uuid
import re
import itertools

import sys
sys.path.append('../../')

__all__ = ['VectorAssembler', 'Tokenizer', 'RegexTokenizer', 'RemoveStopWords',
           'CountVectorizer', 'TfidfVectorizer', 'StringIndexer',
           'IndexToString', 'MaxAbsScaler', 'MinMaxScaler', 'StandardScaler',
           'PCA']


class VectorAssembler(object):

    """Feature Assembler.

    Feature Assembler is a transformer that combines a given list of columns
    into a single vector column. It is useful for combining raw features and
    features generated by different feature transformers into a single feature
    vector, in order to train ML models.

    Feature Assembler accepts the following input column types: all numeric
    types, boolean type, and vector type. In each row, the values of the
    input columns will be concatenated into a vector in the specified order.
    """

    def __init__(self, input_col, output_col=None):

        if len(input_col) == 0:
            raise Exception("You must inform an `input_col` field.")

        if not output_col:
            output_col = 'features'

        self.settings = dict()
        self.settings['inputcol'] = input_col
        self.settings['outputcol'] = output_col

    def transform(self, data):

        def task_vector_assembler(df, params):
            return _feature_assemble_(df, params)

        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = {'name': 'task_vector_assembler',
                                             'status': 'WAIT', 'lazy': True,
                                             'function': [task_vector_assembler,
                                                          self.settings],
                                             'parent': [data.last_uuid],
                                             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


def _feature_assemble_(df, settings):
    """Perform a partial feature assembler."""
    cols = settings['inputcol']
    name = settings['outputcol']

    cols = [col for col in cols if col in df.columns]
    if len(cols) == 0:
        raise Exception("These columns dont belong to this dataset.")

    if len(df) == 0:
        df[name] = np.nan
        return df

    # get the first row to see the type format of each column
    rows = df[cols].iloc[0].values.tolist()
    is_list, not_list = checkfields(rows, cols)

    if len(is_list) == 0:
        df[name] = df[cols].values.tolist()
    else:
        tmp1 = df[not_list].values
        tmp2 = np.array(df[is_list].sum(axis=1).values.tolist())

        df[name] = np.concatenate((tmp1, tmp2), axis=1).tolist()

    return df


def checkfields(rows, cols):
    """Check which fields are a list or a primitive type."""
    is_list = []
    not_list = []
    if len(rows) > 0:
        for item, col_name in zip(rows, cols):
            if isinstance(item, list):
                is_list.append(col_name)
            else:
                not_list.append(col_name)
    return is_list, not_list


class Tokenizer(object):
    """
    Tokenization is the process of taking text (such as a sentence) and
    breaking it into individual terms (usually words). A simple Tokenizer
    class provides this functionality.

    Lazy Function.
    """

    def __init__(self, input_col, output_col=None, min_token_length=2,
                 to_lowercase=True):

        if len(input_col) == 0:
            raise Exception("You must inform an `input_col` field.")

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['inputcol'] = [input_col]
        self.settings['outputcol'] = [output_col]
        self.settings['min_token_length'] = min_token_length
        self.settings['to_lowercase'] = to_lowercase

    def transform(self, data):

        def task_tokenizer(df, params):
            return _tokenizer_(df, params)

        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = {'name': 'tokenizer',
                                             'status': 'WAIT', 'lazy': True,
                                             'function': [task_tokenizer,
                                                          self.settings],
                                             'parent': [data.last_uuid],
                                             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


class RegexTokenizer(object):
    """
    A regex based tokenizer that extracts tokens either by using the provided
    regex pattern (in Java dialect) to split the text (default).

    Lazy Function.
    """

    def __init__(self, input_col, output_col=None, pattern=r'\s+',
                 min_token_length=2, to_lowercase=True):

        if len(input_col) == 0:
            raise Exception("You must inform an `input_col` field.")

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['inputcol'] = [input_col]
        self.settings['outputcol'] = [output_col]
        self.settings['min_token_length'] = min_token_length
        self.settings['to_lowercase'] = to_lowercase
        self.settings['pattern'] = pattern

    def transform(self, data):

        def task_regex_tokenizer(df, params):
            return _tokenizer_(df, params)

        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = {'name': 'regex-tokenizer',
                                             'status': 'WAIT',
                                             'lazy': True,
                                             'function': [task_regex_tokenizer,
                                                          self.settings],
                                             'parent': [data.last_uuid],
                                             'output': 1,
                                             'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


def _tokenizer_(data, settings):
    """Perform a partial tokenizer."""

    input_col = settings['inputcol']
    output_col = settings['outputcol']
    min_token_length = settings['min_token_length']
    to_lowercase = settings['to_lowercase']
    pattern = settings.get('pattern', r'\s+')

    result = []
    for field in data[input_col].values:
        row = []
        for sentence in field:
            toks = re.split(pattern, sentence)
            col = []
            for t in toks:
                if len(t) > min_token_length:
                    if to_lowercase:
                        col.append(t.lower())
                    else:
                        col.append(t)
            row.append(col)
        result.append(row)

    if isinstance(output_col, list):
        for i, col in enumerate(output_col):
            tmp = np.array(result)[:, i]
            if len(tmp) > 0:
                data[col] = tmp
            else:
                data[col] = np.nan
    else:
        data[output_col] = np.ravel(result)

    return data


class RemoveStopWords(object):
    """RemoveStopWordsOperation.

    Stop words are words which should be excluded from the input,
    typically because the words appear frequently and don’t carry
     as much meaning.

    """

    def __init__(self, input_col, output_col=None, case_sensitive=True,
                 stops_words_list=[]):
        if not input_col:
            raise Exception("You must inform the `input_col` field.")

        if not isinstance(input_col, list):
            input_col = [input_col]
            if not output_col:
                output_col = 'col_rm_stopwords'

        else:
            if not output_col:
                output_col = input_col

        self.settings = dict()
        self.settings['news_stops_words'] = stops_words_list
        self.settings['input_col'] = input_col
        self.settings['case_sensitive'] = case_sensitive
        self.settings['output_col'] = output_col

        self.name = 'RemoveStopWords'
        self.stopwords = []

    def stopwords_from_ddf(self, data, input_col):
        if not input_col:
            raise Exception("You must inform the `input_col` field.")

        # It assumes that stopwords's dataframe can fit in memmory
        df = data.partitions[0]
        nfrag = len(df)

        stopwords = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            stopwords[f] = read_stopwords(df[f], input_col)

        stopwords = merge_reduce(merge_stopwords, stopwords)

        self.stopwords = stopwords
        return self

    def transform(self, data):
        """

        :param data: DDF
        :return: DDF
        """

        df = data.partitions[0]
        nfrag = len(df)

        result = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f] = _remove_stopwords(df[f], self.settings, self.stopwords)

        data.partitions = {0: result}
        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = \
            {'name': 'task_transform_stopwords',
             'status': 'COMPLETED',
             'lazy': False,
             'function': result,
             'parent': [data.last_uuid],
             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


@task(returns=list)
def read_stopwords(data1, input_col):
    if len(data1) > 0:
        data1 = np.reshape(data1[input_col], -1, order='C')
    else:
        data1 = np.array([])
    return data1


@task(returns=list)
def merge_stopwords(data1, data2):

    data1 = np.concatenate((data1, data2), axis=0)
    return data1


@task(returns=list)
def _remove_stopwords(data, settings, stopwords):
    """Remove stopwords from a column."""
    columns = settings['input_col']
    alias = settings['output_col']

    # stopwords must be in 1-D
    new_stops = np.reshape(settings['news_stops_words'], -1, order='C')
    if len(stopwords) != 0:
        stopwords = np.concatenate((stopwords, new_stops), axis=0)
    else:
        stopwords = new_stops

    new_data = []
    if data.shape[0] > 0:
        if settings['case_sensitive']:
            stopwords = set(stopwords)
            for index, row in data.iterrows():
                col = []
                for entry in row[columns]:
                    col.append(list(set(entry).difference(stopwords)))
                new_data.append(col)

        else:
            stopwords = [tok.lower() for tok in stopwords]
            stopwords = set(stopwords)

            for index, row in data.iterrows():
                col = []
                for entry in row[columns]:
                    entry = [tok.lower() for tok in entry]
                    col.append(list(set(entry).difference(stopwords)))
                new_data.append(col)

        data[alias] = np.reshape(new_data, -1, order='C')
    return data


class CountVectorizer(object):

    def __init__(self, input_col, output_col, vocab_size=-1, min_tf=1.0,
                 min_df=1.0, binary=True):
        """

        :param input_col:
        :param output_col:
        :param vocab_size: Maximum size of the vocabulary.
                          If -1, no limits will be applied. (default, -1)
        :param min_df: Specifies the minimum number of different documents a
        term must appear in to be included in the vocabulary. If this is an
        integer >= 1, this specifies the number of documents the term must
        appear in;  Default 1.0;
        :param min_df: Filter to ignore rare words in a document. For each
        document, terms with frequency/count less than the given threshold
        are ignored. If this is an integer >= 1, then this specifies a count
        (of times the term must appear in the document);
        :param binary: If True, all nonzero counts are set to 1.
        """
        if not input_col:
            raise Exception("You must inform the `input_col` field.")

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['input_col'] = [input_col]
        self.settings['output_col'] = output_col
        self.settings['vocab_size'] = vocab_size
        self.settings['min_tf'] = min_tf
        self.settings['min_df'] = min_df
        self.settings['binary'] = binary

        self.model = []
        self.name = 'CountVectorizer'

    def fit(self, data):

        vocab_size = self.settings['vocab_size']
        min_tf = self.settings['min_tf']
        min_df = self.settings['min_df']
        df = data.partitions[0]
        nfrag = len(df)

        result_p = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result_p[f] = wordCount(df[f], self.settings)
        word_dic = merge_reduce(merge_wordCount, result_p)
        vocabulary = create_vocabulary(word_dic)

        if any([min_tf > 0, min_df > 0, vocab_size > 0]):
            vocabulary = filter_words(vocabulary, self.settings)

        self.model = [compss_wait_on(vocabulary)]

        return self

    def transform(self, data):
        """

        :param data: DDF
        :return: DDF
        """
        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        vocabulary = self.model[0]

        df = data.partitions[0]
        nfrag = len(df)
        result = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f] = _transform_BoW(df[f], vocabulary, self.settings)

        data.partitions = {0: result}
        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = \
            {'name': 'task_transform_count_vectorizer',
             'status': 'COMPLETED',
             'lazy': False,
             'function': result,
             'parent': [data.last_uuid],
             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


@task(returns=dict)
def wordCount(data, params):
    """Auxilar method to create a model."""
    wordcount = {}
    columns = params['input_col']
    # first:   Number of all occorrences with term t
    # second:  Number of diferent documents with term t
    # third:   temporary - only to idetify the last occorrence

    for i_doc, doc in enumerate(data[columns].values):
        doc = np.array(list(itertools.chain(doc))).flatten()
        for token in doc:
            if token not in wordcount:
                wordcount[token] = [1, 1, i_doc]
            else:
                wordcount[token][0] += 1
                if wordcount[token][2] != i_doc:
                    wordcount[token][1] += 1
                    wordcount[token][2] = i_doc
    return wordcount


@task(returns=dict)
def merge_wordCount(dic1, dic2):
    """Merge the wordcounts."""
    for k in dic2:
        if k in dic1:
            dic1[k][0] += dic2[k][0]
            dic1[k][1] += dic2[k][1]
        else:
            dic1[k] = dic2[k]
    return dic1


@task(returns=list)
def merge_lists(list1, list2):
    """Auxiliar method."""
    list1 = list1+list2
    return list1


@task(returns=list)
def create_vocabulary(word_dic):
    """Create a partial mode."""
    docs_list = [[i[0], i[1][0], i[1][1]] for i in word_dic.items()]
    names = ['Word', 'TotalFrequency', 'DistinctFrequency']
    voc = pd.DataFrame(docs_list, columns=names)\
        .sort_values(by=['Word'])
    return voc


@task(returns=list)
def filter_words(vocabulary, params):
    """Filter words."""
    min_df = params['min_df']
    min_tf = params['min_tf']
    size = params['vocab_size']
    if min_df > 0:
        vocabulary = vocabulary.loc[vocabulary['DistinctFrequency'] >= min_df]
    if min_tf > 0:
        vocabulary = vocabulary.loc[vocabulary['TotalFrequency'] >= min_tf]
    if size > 0:
        vocabulary = vocabulary.sort_values(['TotalFrequency'],
                                            ascending=False). head(size)

    return vocabulary


@task(returns=list)
def _transform_BoW(data, vocabulary, params):
    alias = params['output_col']
    columns = params['input_col']
    binary = params['binary']
    vector = np.zeros((len(data), len(vocabulary)), dtype=np.int)

    vocabulary = vocabulary['Word'].values
    data.reset_index(drop=True, inplace=True)
    for i, point in data.iterrows():
        lines = point[columns].values
        lines = np.array(list(itertools.chain(lines))).flatten()
        for e, w in enumerate(vocabulary):
            if binary:
                if w in lines:
                    vector[i][e] = 1
                else:
                    vector[i][e] = 0
            else:
                vector[i][e] = (lines == w).sum()

    data[alias] = vector.tolist()
    return data


class TfidfVectorizer(object):
    """Term frequency-inverse document frequency (TF-IDF).

    It's a numerical statistic transformation that is intended to reflect
    how important a word is to a document in a collection or corpus.
    """
    def __init__(self, input_col, output_col, vocab_size=-1, min_tf=1.0,
                 min_df=1.0):
        """

        :param input_col:
        :param output_col:
        :param vocab_size: Maximum size of the vocabulary.
                          If -1, no limits will be applied. (default, -1)
        :param min_df: Specifies the minimum number of different documents a
        term must appear in to be included in the vocabulary. If this is an
        integer >= 1, this specifies the number of documents the term must
        appear in;  Default 1.0;
        :param min_df: Filter to ignore rare words in a document. For each
        document, terms with frequency/count less than the given threshold
        are ignored. If this is an integer >= 1, then this specifies a count
        (of times the term must appear in the document);
        """
        if not input_col:
            raise Exception("You must inform the `input_col` field.")

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['input_col'] = [input_col]
        self.settings['output_col'] = output_col
        self.settings['vocab_size'] = vocab_size
        self.settings['min_tf'] = min_tf
        self.settings['min_df'] = min_df

        self.model = []
        self.name = 'TfidfVectorizer'

    def fit(self, data):

        vocab_size = self.settings['vocab_size']
        min_tf = self.settings['min_tf']
        min_df = self.settings['min_df']
        df = data.partitions[0]
        nfrag = len(df)

        result_p = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result_p[f] = wordCount(df[f], self.settings)
        word_dic = merge_reduce(merge_wordCount, result_p)
        vocabulary = create_vocabulary(word_dic)

        if any([min_tf > 0, min_df > 0, vocab_size > 0]):
            vocabulary = filter_words(vocabulary, self.settings)

        self.model = [compss_wait_on(vocabulary)]

        return self

    def transform(self, data):
        """

        :param data: DDF
        :return: DDF
        """

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")
        vocabulary = self.model[0]

        df = data.partitions[0]
        nfrag = len(df)

        counts = [count_records(df[f]) for f in range(nfrag)]
        count = merge_reduce(mergeCount, counts)

        result = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f] = \
                construct_tf_idf(df[f], vocabulary, self.settings, count)

        data.partitions = {0: result}
        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = \
            {'name': 'task_transform_tfidf',
             'status': 'COMPLETED',
             'lazy': False,
             'function': result,
             'parent': [data.last_uuid],
             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


@task(returns=list)
def count_records(data):
    """Count the partial number of records in each fragment."""
    return len(data)


@task(returns=list)
def mergeCount(data1, data2):
    """Auxiliar method to merge the lengths."""
    return data1 + data2


@task(returns=list)
def construct_tf_idf(data, vocabulary, params, num_doc):
    """Construct the tf-idf feature.

    TF(t)  = (Number of times term t appears in a document)
                    / (Total number of terms in the document).
    IDF(t) = log( Total number of documents /
                    Number of documents with term t in it).
    Source: http://www.tfidf.com/
    """

    alias = params['output_col']
    columns = params['input_col']
    vector = np.zeros((data.shape[0], vocabulary.shape[0]), dtype=np.float)
    vocab = vocabulary['Word'].values
    data.reset_index(drop=True, inplace=True)

    for i, point in data.iterrows():
        lines = point[columns].values
        lines = np.array(list(itertools.chain(lines))).flatten()
        for w, token in enumerate(vocab):
            if token in lines:
                # TF = (Number of times term t appears in the document) /
                #        (Total number of terms in the document).
                nTimesTermT = np.count_nonzero(lines == token)
                total = len(lines)
                if total > 0:
                    tf = float(nTimesTermT) / total
                else:
                    tf = 0

                # IDF = log_e(Total number of documents /
                #            Number of documents with term t in it).
                nDocsWithTermT = vocabulary.\
                    loc[vocabulary['Word'] == token, 'DistinctFrequency'].\
                    item()
                idf = np.log(float(num_doc) / nDocsWithTermT)

                vector[i][w] = tf*idf

    data[alias] = vector.tolist()

    return data


class MinMaxScaler(object):
    """MinMax Scaler Operation.

    MinMaxScaler transforms a dataset of features rows, rescaling
    each feature to a specific range (often [0, 1])

    The rescaled value for a feature E is calculated as,

    Rescaled(ei) = (ei − Emin)∗(max − min)/(Emax − Emin) + min

    For the case Emax == Emin,  Rescaled(ei) = 0.5∗(max + min)

    """

    def __init__(self, input_col, output_col, feature_range=(0, 1)):
        if not input_col:
            raise Exception("You must inform the `input_col` field.")

        if not output_col:
            output_col = input_col

        if not isinstance(feature_range, tuple) or \
                feature_range[0] >= feature_range[1]:
            raise Exception("You must inform a valid `feature_range`.")

        self.settings = dict()
        self.settings['input_col'] = [input_col]
        self.settings['output_col'] = [output_col]
        self.settings['feature_range'] = feature_range

        self.model = []
        self.name = 'MinMaxScaler'

    def fit(self, data):

        df = data.partitions[0]
        nfrag = len(df)
        columns = self.settings['input_col']
        # generate a list of the min and the max element to each subset.
        minmax_partial = \
            [_agg_maxmin(df[f], columns) for f in range(nfrag)]

        # merge them into only one list
        minmax = merge_reduce(_merge_maxmin, minmax_partial)

        self.model = [minmax]
        return self

    def transform(self, data):

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        nfrag = len(data.partitions[0])
        result = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f] = _minmax_scaler(data.partitions[0][f],
                                       self.settings, self.model[0])

        data.partitions = {0: result}
        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = \
            {'name': 'task_transform_minmax_scaler',
             'status': 'COMPLETED',
             'lazy': False,
             'function': result,
             'parent': [data.last_uuid],
             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


@task(returns=list)
def _agg_maxmin(df, columns):
    """Generate a list of min and max values, excluding NaN values."""
    min_max_p = []
    for col in columns:
        p = [np.min(df[col].values, axis=0), np.max(df[col].values, axis=0)]
        min_max_p.append(p)
    return min_max_p


@task(returns=list)
def _merge_maxmin(minmax1, minmax2):
    """Merge min and max values."""
    minmax = []
    for feature in zip(minmax1, minmax2):
        di, dj = feature
        minimum = di[0] if di[0] < dj[0] else dj[0]
        maximum = di[1] if di[1] > dj[1] else dj[1]
        minmax.append([minimum, maximum])
    return minmax


@task(returns=list)
def _minmax_scaler(data, settings, minmax):
    """Normalize by min max mode."""
    features = settings['input_col']
    alias = settings.get('output_col', [])
    min_r, max_r = settings.get('feature_range', (0, 1))

    if len(alias) != len(features):
        alias = features

    def calculation(xs, minimum, maximum, min_r, max_r):
        features = []
        diff_r = float(max_r - min_r)
        for xi, mi, ma in zip(xs, minimum, maximum):
            if ma == mi:
                v = 0.5 * (max_r + min_r)
            else:
                v = (float(xi - mi) * (diff_r / (ma - mi))) + min_r
            features.append(v)
        return features

    for i, (alias, col) in enumerate(zip(alias, features)):
        minimum, maximum = minmax[i]
        data[alias] = data[col].apply(
                lambda xs: calculation(xs, minimum, maximum, min_r, max_r))
    return data


class MaxAbsScaler(object):
    """MaxAbs Scaler Operation.

    MaxAbsScaler transforms a dataset of features rows,
    rescaling each feature to range [-1, 1] by dividing through
    the maximum absolute value in each feature.

    This estimator scales and translates each feature individually
    such that the maximal absolute value of each feature in the
    training set will be 1.0. It does not shift/center the data,
    and thus does not destroy any sparsity.
    """

    def __init__(self, input_col, output_col):
        if not input_col:
            raise Exception("You must inform the `input_col` field.")

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['input_col'] = [input_col]
        self.settings['output_col'] = [output_col]

        self.model = []
        self.name = 'MaxAbsScaler'

    def fit(self, data):

        df = data.partitions[0]
        nfrag = len(df)
        columns = self.settings['input_col']
        # generate a list of the min and the max element to each subset.
        minmax_partial = \
            [_agg_maxabs(df[f], columns) for f in range(nfrag)]

        # merge them into only one list
        minmax = merge_reduce(_merge_maxabs, minmax_partial)

        self.model = [minmax]
        return self

    def transform(self, data):

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        nfrag = len(data.partitions[0])
        result = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f] = _maxabs_scaler(data.partitions[0][f],
                                       self.model[0], self.settings)

        data.partitions = {0: result}
        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = \
            {'name': 'task_transform_maxabs_scaler',
             'status': 'COMPLETED',
             'lazy': False,
             'function': result,
             'parent': [data.last_uuid],
             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


@task(returns=list)
def _agg_maxabs(df, columns):
    """Generate a list of min and max values, excluding NaN values."""
    min_max_p = []

    if len(df) > 0:
        for col in columns:
            p = [np.min(df[col].values, axis=0), np.max(df[col].values, axis=0)]
            min_max_p.append(p)

    return min_max_p


@task(returns=list)
def _merge_maxabs(minmax1, minmax2):
    """Merge max abs values."""
    maxabs = []
    if len(minmax1) > 0 and len(minmax2) > 0:
        for feature in zip(minmax1, minmax2):
            d_esq, d_dir = feature

            minimum = [min([di, dj]) for di, dj in zip(d_esq[0], d_dir[0])]
            maximum = [max([di, dj]) for di, dj in zip(d_esq[1], d_dir[1])]
            maxabs.append([minimum, maximum])
    elif len(minmax1) > len(minmax2):
        maxabs = minmax1
    else:
        maxabs = minmax2

    return maxabs


@task(returns=list)
def _maxabs_scaler(data, minmax, settings):
    """Normalize by range mode."""
    features = settings['input_col']
    alias = settings.get('output_col', [])

    if len(alias) != len(features):
        alias = features

    def calculation(xs, minimum, maximum):
        features = []
        for xi, mi, ma in zip(xs, minimum, maximum):
            ma = abs(ma)
            mi = abs(mi)
            maxabs = ma if ma > mi else mi
            v = float(xi) / maxabs
            features.append(v)
        return features

    for i, (alias, col) in enumerate(zip(alias, features)):
        minimum, maximum = minmax[i]
        data[alias] = data[col].apply(
                lambda xs: calculation(xs, minimum, maximum))

    return data


class StandardScaler(object):
    """The standard score of a sample x is calculated as:

        z = (x - u) / s
        where u is the mean of the training samples or zero if
        with_mean=False, and s is the standard deviation of the
        training samples or one if with_std=False.
    """

    def __init__(self, input_col, output_col=None,
                 with_mean=True, with_std=True):

        if not input_col:
            raise Exception("You must inform the `input_col` field.")

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['input_col'] = [input_col]
        self.settings['output_col'] = [output_col]
        self.settings['with_mean'] = with_mean
        self.settings['with_std'] = with_std

        self.model = []
        self.name = 'StandardScaler'

    def fit(self, data):
        """

        :param data: DDF
        :return: trained model
        """

        df = data.partitions[0]
        nfrag = len(df)

        features = self.settings['input_col']

        # compute the sum of each subset column
        sum_partial = \
            [_agg_sum(df[f], features) for f in range(nfrag)]
        # merge then to compute a mean
        mean = merge_reduce(_merge_sum, sum_partial)

        # using this mean, compute the variance of each subset column
        sse_partial = \
            [_agg_sse(df[f], features, mean) for f in range(nfrag)]
        sse = merge_reduce(_merge_sse, sse_partial)

        self.model = [[mean, sse]]

        return self

    def transform(self, data):
        """

        :param data: DDF
        :return: DDF
        """

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        df = data.partitions[0]
        nfrag = len(df)

        mean, sse = self.model[0]
        result = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f] = _stardard_scaler(df[f], self.settings, mean, sse)

        data.partitions = {0: result}
        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = \
            {'name': 'task_transform_standard_scaler',
             'status': 'COMPLETED',
             'lazy': False,
             'function': result,
             'parent': [data.last_uuid],
             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


@task(returns=list)
def _agg_sum(df, features):
    """Pre-compute some values."""
    sum_partial = []
    for feature in features:
        sum_p = [np.nansum(df[feature].values.tolist(), axis=0),
                 len(df[feature])]
        sum_partial.append(sum_p)
    return sum_partial


@task(returns=list, priority=True)
def _merge_sum(sum1, sum2):
    """Merge pre-computation."""
    sum_count = []
    for f_i, f_j in zip(sum1, sum2):
        count = f_i[1] + f_j[1]
        sums = []
        for di, dj in zip(f_i[0], f_j[0]):
            sum_f = di + dj
            sums.append(sum_f)
        sum_count.append([sums, count])

    return sum_count


@task(returns=list)
def _agg_sse(df, features, sum_count):
    """Perform a partial SSE calculation."""
    sum_sse = []

    def computation_sse(xs, means):
        sse = []
        for xi, mi in zip(xs, means):
            sse.append((xi - mi) ** 2)
        return sse

    for sum_f, col in zip(sum_count, features):
        size = sum_f[1]
        sums = sum_f[0]
        means = [float(x) / size for x in sums]

        sum_sse.append(
                np.nansum(df[col].apply(lambda xs: computation_sse(xs, means))
                          .values.tolist(), axis=0))

    return sum_sse


@task(returns=list, priority=True)
def _merge_sse(sum1, sum2):
    """Merge the partial SSE."""
    sum_count = []
    for di, dj in zip(sum1, sum2):
        sum_count.append(di + dj)
    return sum_count


@task(returns=list)
def _stardard_scaler(data, settings, mean, sse):
    """Normalize by Standard mode."""
    features = settings['input_col']
    alias = settings['output_col']
    with_mean = settings['with_mean']
    with_std = settings['with_std']

    if len(alias) != len(features):
        alias = features

    def computation_scaler(xs, means, stds):
        scaler = []
        for xi, mi, std in zip(xs, means, stds):
            scaler.append(float(xi - mi) / std)
        return scaler

    for i, (alias, col) in enumerate(zip(alias, features)):

        size = mean[i][1]

        if with_mean:
            means = [float(x) / size for x in mean[i][0]]
        else:
            means = [0 for _ in mean[i][0]]

        if with_std:
            stds = [np.sqrt(float(sse_p) / size) for sse_p in sse[i]]  # std pop
        else:
            stds = [1.0 for _ in sse[i]]

        data[alias] = data[col] \
            .apply(lambda xs: computation_scaler(xs, means, stds))

    return data


class StringIndexer(ModelDDS):
    """String Indexer.

    Indexes a feature by encoding a string column as a
    column containing indexes.
    """

    def __init__(self, input_col, output_col=None):

        super(StringIndexer, self).__init__()

        if not input_col:
            raise Exception("You must inform the `input_col` field.")

        if not output_col:
            output_col = "{}_indexed".format(input_col)

        self.settings = dict()
        self.settings['input_col'] = input_col
        self.settings['output_col'] = output_col

        self.model = []
        self.name = 'StringIndexer'

    def fit(self, data):
        """Validation step and inital settings."""

        in_col = self.settings['input_col']

        df = data.partitions[0]
        nfrag = len(df)

        mapper = [get_indexes(df[f], in_col) for f in range(nfrag)]
        mapper = merge_reduce(merge_mapper, mapper)

        self.model = [compss_wait_on(mapper)]
        return self

    def transform(self, data):

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        in_col = self.settings['input_col']
        out_col = self.settings['output_col']

        df = data.partitions[0]
        nfrag = len(df)

        result = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f] = _string_to_indexer(df[f], in_col, out_col,
                                           self.model[0])

        data.partitions = {0: result}
        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = \
            {'name': 'task_transform_string_indexer',
             'status': 'COMPLETED',
             'lazy': False,
             'function': result,
             'parent': [data.last_uuid],
             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


@task(returns=list)
def get_indexes(data, in_col):
    """Create partial model to convert string to index."""
    return data[in_col].dropna().unique()


@task(returns=list)
def merge_mapper(data1, data2):
    """Merge partial models into one."""
    data1 = np.concatenate((data1, data2), axis=0)
    return np.unique(data1)


@task(returns=list)
def _string_to_indexer(data, in_col, out_col, mapper):
    """Convert string to index based in the model."""
    news = [i for i in range(len(mapper))]
    mapper = mapper.tolist()
    data[out_col] = data[in_col].replace(to_replace=mapper, value=news)
    return data


class IndexToString(ModelDDS):
    """Feature Indexer.

    Symmetrically to StringIndexer, IndexToString maps a column of
    label indices back to a column containing the original labels as strings.
    """

    def __init__(self, input_col, model, output_col=None):

        super(IndexToString, self).__init__()

        if not input_col:
            raise Exception("You must inform the `input_col` field.")

        if not output_col:
            output_col = "{}_converted".format(input_col)

        self.settings = dict()
        self.settings['input_col'] = input_col
        self.settings['output_col'] = output_col

        self.model = [model.model[0]]
        self.name = 'IndexToString'

    def transform(self, data):

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        inputCol = self.settings['input_col']
        outputCol = self.settings['output_col']

        df = data.partitions[0]
        nfrag = len(df)

        result = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f] = _index_to_string(df[f], inputCol,
                                           outputCol, self.model[0])

        data.partitions = {0: result}
        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = \
            {'name': 'task_transform_indextostring',
             'status': 'COMPLETED',
             'lazy': False,
             'function': result,
             'parent': [data.last_uuid],
             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


@task(returns=list)
def _index_to_string(data, inputCol, outputCol, mapper):
    """Convert index to string based in the model."""
    news = [i for i in range(len(mapper))]
    mapper = mapper.tolist()
    data[outputCol] = data[inputCol].replace(to_replace=news, value=mapper)
    return data


class PCA(ModelDDS):
    """PCA.

    Principal component analysis (PCA) is a statistical method to find
    a rotation such that the first coordinate has the largest variance
    possible, and each succeeding coordinate in turn has the largest
    variance possible. The columns of the rotation matrix are called
    principal components. PCA is used widely in dimensionality reduction.
    """

    def __init__(self, input_col, n_components, output_col=None):

        if not output_col:
            output_col = 'prediction_PCA'

        self.settings = dict()
        self.settings['input_col'] = input_col
        self.settings['output_col'] = output_col
        self.settings['n_components'] = n_components

        self.model = []
        self.name = 'PCA'

    def fit(self, data):
        """

        :param data: DDF
        :return: trained model
        """

        df = data.partitions[0]
        nfrag = len(df)

        NComponents = self.settings.get('n_components')
        cols = self.settings.get('input_col')

        partial_count = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            partial_count[f] = pca_count(df[f], cols)

        mergedCount = merge_reduce(pca_mergeCount, partial_count)
        mergedCount = meanCalc(mergedCount)
        for f in range(nfrag):
            partial_count[f] = partial_multiply(df[f], cols, mergedCount)

        mergedcov = merge_reduce(pca_mergeCov, partial_count)

        info = pca_eigendecomposition(mergedcov, mergedCount, NComponents)
        info = compss_wait_on(info)

        self.model = dict()
        self.model['algorithm'] = 'PCA'
        self.model['cum_var_exp'] = info[0]
        self.model['eig_vals'] = info[1]
        self.model['eig_vecs'] = info[2]
        self.model['model'] = info[3]

        return self

    def transform(self, data):
        """Transform.

        - :param data:      A list with nfrag pandas's dataframe.
        - :param model:		The pca model created;
        - :param settings:  A dictionary that contains:
            - features: 	Field of the features data;
            - predCol:    	Alias to the new features field;
        - :param nfrag:   A number of fragments;
        - :return:          A list with nfrag pandas's dataframe
                            (in the same input format).
        """
        df = data.partitions[0]
        nfrag = len(df)

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        model = self.model['model']
        features_col = self.settings['input_col']
        predCol = self.settings['output_col']

        result = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f] = _pca_transform(df[f], features_col, predCol, model)

        data.partitions = {0: result}
        uuid_key = str(uuid.uuid4())
        COMPSsContext.tasks_map[uuid_key] = \
            {'name': 'task_transform_pca',
             'status': 'COMPLETED',
             'lazy': False,
             'function': result,
             'parent': [data.last_uuid],
             'output': 1, 'input': 1}

        data.set_n_input(uuid_key, data.settings['input'])
        return DDF(data.partitions, data.task_list, uuid_key)


@task(returns=list)
def pca_count(data, cols):
    """Partial count."""
    N = len(data)
    partialsum = 0
    if N > 0:
        data = data[cols].values
        partialsum = reduce(lambda l1, l2: np.add(l1, l2), data)
    return [N, partialsum]


@task(returns=list)
def pca_mergeCount(count1, count2):
    """Merge partial counts."""
    N = count1[0] + count2[0]
    partialsum = np.add(count1[1], count2[1])
    return [N, partialsum]


# @local
@task(returns=list, priority=True)
def meanCalc(mergedCount):
    """Generate the mean value."""
    # This method could be executed nfrag times inside each next function
    # with this, we can remove this function

    N = mergedCount[0]
    mean = np.array(map(lambda x: float(x)/N, mergedCount[1]))
    return [mean, N]


@task(returns=list)
def partial_multiply(data, col, mean):
    """Perform partial calculation."""
    cov_mat = 0

    if len(data) > 0:
        mean_vec = mean[0]
        X_std = data[col].values
        X_std = np.array(X_std.tolist())

        first_part = X_std - mean_vec
        cov_mat = first_part.T.dot(first_part)

    return cov_mat


@task(returns=list)
def pca_mergeCov(cov1, cov2):
    """Merge covariance."""
    return np.add(cov1, cov2)


# @local
@task(returns=list, priority=True)
def pca_eigendecomposition(cov_mat, mean, n_components):
    """Generate a eigen decomposition."""
    N = mean[1]
    M = len(cov_mat)
    cov_mat = cov_mat / (N-1)
    eig_vals, eig_vecs = np.linalg.eig(cov_mat)

    # Sort the eigenvalue and vecs tuples from high to low
    inds = eig_vals.argsort()[::-1]
    sortedEighVals = eig_vals[inds]
    sortedEighVecs = eig_vecs[inds]

    tot = sum(eig_vals)
    var_exp = [(i / tot)*100 for i in sortedEighVals]
    cum_var_exp = np.cumsum(var_exp)

    n_components = min([n_components, M])

    matrix_w = sortedEighVecs[:, 0:n_components]

    return [cum_var_exp, sortedEighVals, sortedEighVecs, matrix_w]


@task(returns=list)
def _pca_transform(data, features, predCol, model):
    """Reduce the dimensionality based in the created model."""
    tmp = []
    if len(data) > 0:
        tmp = np.array(data[features].values.tolist()).dot(model).tolist()

    data[predCol] = tmp
    return data