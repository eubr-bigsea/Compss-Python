#!/usr/bin/python
# -*- coding: utf-8 -*-

__author__ = "Lucas Miguel S Ponce"
__email__ = "lucasmsp@gmail.com"

from pycompss.api.task import task
from pycompss.functions.reduce import merge_reduce
from pycompss.api.api import compss_wait_on
from ddf_library.ddf import DDF, DDFSketch
from ddf_library.ddf_model import ModelDDF
from pycompss.api.local import *
import numpy as np
import pandas as pd
import re
import itertools

import sys
sys.path.append('../../')

preprocessing = ['Binarizer', 'StringIndexer', 'IndexToString', 'OneHotEncoder',
                 'MaxAbsScaler', 'MinMaxScaler', 'StandardScaler']

text_operations = ['Tokenizer', 'RegexTokenizer', 'RemoveStopWords',
                   'NGram', 'CountVectorizer', 'TfidfVectorizer']

__all__ = ['VectorAssembler', 'VectorSlicer', 'PCA', 'PolynomialExpansion']\
          + preprocessing + text_operations


class VectorAssembler(object):

    """
    Vector Assembler is a transformer that combines a given list of columns
    into a single vector column. It is useful for combining raw features and
    features generated by different feature transformers into a single feature
    vector, in order to train ML models.

    Vector Assembler accepts the following input column types: all numeric
    types, boolean type, and vector type. In each row, the values of the
    input columns will be concatenated into a vector in the specified order.

    :Example:

    >>> assembler = VectorAssembler(input_col=["x", "y"], output_col="features")
    >>> ddf2 = assembler.transform(ddf1)
    """

    def __init__(self, input_col, output_col=None):
        """
        :param input_col: List of columns;
        :param output_col: Output column name.
        """

        if not output_col:
            output_col = 'features'

        self.settings = dict()
        self.settings['inputcol'] = input_col
        self.settings['outputcol'] = output_col

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        def task_vector_assembler(df, params):
            return _feature_assemble_(df, params)

        uuid_key = data._ddf_add_task(task_name='vector_assembler',
                                      status='WAIT', lazy=True,
                                      function=[task_vector_assembler,
                                                self.settings],
                                      parent=[data.last_uuid],
                                      n_output=1, n_input=1)

        data._set_n_input(uuid_key, data.settings['input'])
        return DDF(task_list=data.task_list, last_uuid=uuid_key)


def _feature_assemble_(df, settings):
    """Perform a partial feature assembler."""
    cols = settings['inputcol']
    name = settings['outputcol']

    cols = [col for col in cols if col in df.columns]
    if len(cols) == 0:
        raise Exception("These columns dont belong to this dataset.")

    if len(df) == 0:
        df[name] = np.nan
    else:

        # get the first row to see the type format of each column
        rows = df[cols].iloc[0].values.tolist()
        is_list, not_list = checkfields(rows, cols)

        if len(is_list) == 0:
            df[name] = df[cols].values.tolist()
        else:
            tmp1 = df[not_list].values
            tmp2 = np.array(df[is_list].sum(axis=1).values.tolist())

            df[name] = np.concatenate((tmp1, tmp2), axis=1).tolist()

    info = [df.columns.tolist(), df.dtypes.values, [len(df)]]
    return df, info


def checkfields(rows, cols):
    """Check which fields are a list or a primitive type."""
    is_list = []
    not_list = []
    if len(rows) > 0:
        for item, col_name in zip(rows, cols):
            if isinstance(item, list):
                is_list.append(col_name)
            else:
                not_list.append(col_name)
    return is_list, not_list


class VectorSlicer(object):
    """
    Vector Slicer class takes a feature vector and outputs a new feature
    vector with a subarray of the original features.

    The output vector will order features with the selected indices first
    (in the order given), followed by the selected names (in the order given).

    :Example:

    >>> ddf = VectorSlicer(input_col='features', indices=[0, 5).transform(ddf)
    """

    def __init__(self, input_col, output_col=None, indices=None):
        """
        :param input_col: Feature vector field.
        :param output_col: New feature vector field (overwrite by default)
        :param indices: indices list
        """
        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['inputcol'] = input_col
        self.settings['outputcol'] = output_col

        if indices is None:
            raise Exception('At least one feature must be selected.')

        self.settings['indices'] = indices

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        def task_vector_slicer(df, params):
            return _vector_slicer(df, params)

        uuid_key = data._ddf_add_task(task_name='vector_slicer',
                                      status='WAIT', lazy=True,
                                      function=[task_vector_slicer,
                                                self.settings],
                                      parent=[data.last_uuid],
                                      n_output=1, n_input=1)

        data._set_n_input(uuid_key, data.settings['input'])
        return DDF(task_list=data.task_list, last_uuid=uuid_key)


def _vector_slicer(df, settings):

    output_col = settings['outputcol']

    if len(df) == 0:
        df[output_col] = np.nan

    else:

        input_col = settings['inputcol']
        idx = settings['indices']

        values = np.array(df[input_col].tolist())[:, idx]
        df[output_col] = values.tolist()

    info = [df.columns.tolist(), df.dtypes.values, [len(df)]]
    return df, info


class Binarizer(object):
    """
    Binarize data (set feature values to 0 or 1) according to a threshold

    Values greater than the threshold map to 1, while values less than or equal
    to the threshold map to 0. With the default threshold of 0, only positive
    values map to 1.

    :Example:

    >>> ddf = Binarizer(input_col='features', threshold=5.0).transform(ddf)
    """

    def __init__(self, input_col, output_col=None, threshold=0.0):
        """
        :param input_col: List of columns;
        :param output_col: Output column name.
        :param threshold: Feature values below or equal to this are
         replaced by 0, above it by 1. Default = 0.0.
        """

        if not output_col:
            output_col = 'features'

        self.settings = dict()
        self.settings['inputcol'] = input_col
        self.settings['outputcol'] = output_col
        self.settings['threshold'] = threshold

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        def task_binarizer(df, params):
            return _binarizer(df, params)

        uuid_key = data._ddf_add_task(task_name='binarizer',
                                      status='WAIT', lazy=True,
                                      function=[task_binarizer,
                                                self.settings],
                                      parent=[data.last_uuid],
                                      n_output=1, n_input=1)

        data._set_n_input(uuid_key, data.settings['input'])
        return DDF(task_list=data.task_list, last_uuid=uuid_key)


def _binarizer(df, settings):
    output_col = settings['outputcol']

    if len(df) == 0:
        df[output_col] = np.nan

    else:
        input_col = settings['inputcol']
        threshold = settings['threshold']

        values = np.array(df[input_col].tolist())
        from sklearn.preprocessing import Binarizer
        values = Binarizer(threshold=threshold).fit_transform(values)
        df[output_col] = values.tolist()

    info = [df.columns.tolist(), df.dtypes.values, [len(df)]]
    return df, info


class Tokenizer(object):
    """
    Tokenization is the process of taking text (such as a sentence) and
    breaking it into individual terms (usually words). A simple Tokenizer
    class provides this functionality.

    :Example:

    >>> ddf2 = Tokenizer(input_col='features').transform(ddf1)
    """

    def __init__(self, input_col, output_col=None, min_token_length=2,
                 to_lowercase=True):
        """
        :param input_col: Input column with sentences;
        :param output_col: Output column (overwrite the *'input_col'* if None);
        :param min_token_length: Minimum tokens length (default is 2);
        :param to_lowercase: To convert words to lowercase (default is True).
        """

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['inputcol'] = [input_col]
        self.settings['outputcol'] = [output_col]
        self.settings['min_token_length'] = min_token_length
        self.settings['to_lowercase'] = to_lowercase

    def transform(self, data):
        """

        :param data: DDF
        :return: DDF
        """

        def task_tokenizer(df, params):
            return _tokenizer_(df, params)

        uuid_key = data._ddf_add_task(task_name='tokenizer',
                                      status='WAIT', lazy=True,
                                      function=[task_tokenizer,
                                                self.settings],
                                      parent=[data.last_uuid],
                                      n_output=1, n_input=1)

        data._set_n_input(uuid_key, data.settings['input'])
        return DDF(task_list=data.task_list, last_uuid=uuid_key)


class RegexTokenizer(object):
    """
    A regex based tokenizer that extracts tokens either by using the provided
    regex pattern (in Java dialect) to split the text.

    :Example:

    >>> ddf2 = RegexTokenizer(input_col='col_0', pattern=r'\s+').transform(ddf1)
    """

    def __init__(self, input_col, output_col=None, pattern=r'\s+',
                 min_token_length=2, to_lowercase=True):
        """
        :param input_col: Input column with sentences;
        :param output_col: Output column (overwrite the *'input_col'* if None);
        :param pattern: Regex pattern in Java dialect, default is *r'\s+'*;
        :param min_token_length: Minimum tokens length (default is 2);
        :param to_lowercase: To convert words to lowercase (default is True).
        """

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['inputcol'] = [input_col]
        self.settings['outputcol'] = [output_col]
        self.settings['min_token_length'] = min_token_length
        self.settings['to_lowercase'] = to_lowercase
        self.settings['pattern'] = pattern

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        def task_regex_tokenizer(df, params):
            return _tokenizer_(df, params)

        uuid_key = data._ddf_add_task(task_name='tokenizer',
                                      status='WAIT', lazy=True,
                                      function=[task_regex_tokenizer,
                                                self.settings],
                                      parent=[data.last_uuid],
                                      n_output=1, n_input=1)

        data._set_n_input(uuid_key, data.settings['input'])
        return DDF(task_list=data.task_list, last_uuid=uuid_key)


def _tokenizer_(data, settings):
    """Perform a partial tokenizer."""

    input_col = settings['inputcol']
    output_col = settings['outputcol']
    min_token_length = settings['min_token_length']
    to_lowercase = settings['to_lowercase']
    pattern = settings.get('pattern', r'\s+')

    result = []
    for field in data[input_col].values:
        row = []
        for sentence in field:
            toks = re.split(pattern, sentence)
            col = []
            for t in toks:
                if len(t) > min_token_length:
                    if to_lowercase:
                        col.append(t.lower())
                    else:
                        col.append(t)
            row.append(col)
        result.append(row)

    if isinstance(output_col, list):
        for i, col in enumerate(output_col):
            tmp = np.array(result)[:, i]
            if len(tmp) > 0:
                data[col] = tmp
            else:
                data[col] = np.nan
    else:
        data[output_col] = np.ravel(result)

    info = [data.columns.tolist(), data.dtypes.values, [len(data)]]
    return data, info


class RemoveStopWords(DDFSketch):
    """
    Remove stop-words is a operation to remove words which
    should be excluded from the input, typically because
    the words appear frequently and don’t carry as much meaning.

    :Example:

    >>> remover = RemoveStopWords(input_col='col_0', output_col='col_1',
    >>>                           stops_words_list=['rock', 'destined'])
    >>> remover = remover.stopwords_from_ddf(stopswords_ddf, 'col')
    >>> ddf2 = remover.transform(ddf1)
    """

    def __init__(self, input_col, output_col=None, case_sensitive=True,
                 stops_words_list=None):
        """
        :param input_col: Input columns with the tokens;
        :param output_col: Output column;
        :param case_sensitive: To compare words using case sensitive (default);
        :param stops_words_list: Optional, a list of words to be removed.
        """
        super(RemoveStopWords, self).__init__()

        if not isinstance(input_col, list):
            input_col = [input_col]
            if not output_col:
                output_col = 'col_rm_stopwords'

        else:
            if not output_col:
                output_col = input_col

        self.settings = dict()
        self.settings['news_stops_words'] = stops_words_list
        self.settings['input_col'] = input_col
        self.settings['case_sensitive'] = case_sensitive
        self.settings['output_col'] = output_col

        self.name = 'RemoveStopWords'
        self.stopwords = []

    def stopwords_from_ddf(self, data, input_col):
        """
        Is also possible inform stop-words form a DDF.

        :param data: DDF with a column of stop-words;
        :param input_col: Stop-words column name;
        """

        # It assumes that stopwords's dataframe can fit in memmory
        df, nfrag, tmp = self._ddf_inital_setup(data)

        stopwords = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            stopwords[f] = read_stopwords(df[f], input_col)

        stopwords = merge_reduce(merge_stopwords, stopwords)

        self.stopwords = stopwords
        return self

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        df, nfrag, tmp = self._ddf_inital_setup(data)

        result = [[] for _ in range(nfrag)]
        info = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f], info[f] = _remove_stopwords(df[f], self.settings,
                                                   self.stopwords)

        uuid_key = self._ddf_add_task(task_name='task_transform_stopwords',
                                      status='COMPLETED', lazy=False,
                                      function={0: result},
                                      parent=[tmp.last_uuid],
                                      n_output=1, n_input=1, info=info)

        self._set_n_input(uuid_key, 0)
        return DDF(task_list=tmp.task_list, last_uuid=uuid_key)


@task(returns=list)
def read_stopwords(data1, input_col):
    if len(data1) > 0:
        data1 = np.reshape(data1[input_col], -1, order='C')
    else:
        data1 = np.array([])
    return data1


@task(returns=list)
def merge_stopwords(data1, data2):

    data1 = np.concatenate((data1, data2), axis=0)
    return data1


@task(returns=2)
def _remove_stopwords(data, settings, stopwords):
    """Remove stopwords from a column."""
    columns = settings['input_col']
    alias = settings['output_col']

    # stopwords must be in 1-D
    new_stops = np.reshape(settings['news_stops_words'], -1, order='C')
    if len(stopwords) != 0:
        stopwords = np.concatenate((stopwords, new_stops), axis=0)
    else:
        stopwords = new_stops

    new_data = []
    if data.shape[0] > 0:
        if settings['case_sensitive']:
            stopwords = set(stopwords)
            for index, row in data.iterrows():
                col = []
                for entry in row[columns]:
                    col.append(list(set(entry).difference(stopwords)))
                new_data.append(col)

        else:
            stopwords = [tok.lower() for tok in stopwords]
            stopwords = set(stopwords)

            for index, row in data.iterrows():
                col = []
                for entry in row[columns]:
                    entry = [tok.lower() for tok in entry]
                    col.append(list(set(entry).difference(stopwords)))
                new_data.append(col)

        data[alias] = np.reshape(new_data, -1, order='C')

    info = [data.columns.tolist(), data.dtypes.values, [len(data)]]
    return data, info


class NGram(object):
    """
    A feature transformer that converts the input array of strings into an
    array of n-grams. Null values in the input array are ignored. It returns
    an array of n-grams where each n-gram is represented by a space-separated
    string of words. When the input is empty, an empty array is returned. When
    the input array length is less than n (number of elements per n-gram), no
    n-grams are returned.
    """

    def __init__(self, input_col, n=2, output_col=None):
        """

        :param input_col: Input columns with the tokens;
        :param n: Number integer. Default = 2;
        :param output_col: Output column. Default, overwrite input_col;

        """
        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['inputcol'] = input_col
        self.settings['outputcol'] = output_col
        self.settings['n'] = n

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        def task_ngram(df, params):
            return _ngram(df, params)

        uuid_key = data._ddf_add_task(task_name='ngram',
                                      status='WAIT', lazy=True,
                                      function=[task_ngram, self.settings],
                                      parent=[data.last_uuid],
                                      n_output=1, n_input=1)

        data._set_n_input(uuid_key, data.settings['input'])
        return DDF(task_list=data.task_list, last_uuid=uuid_key)


def _ngram(df, settings):

    output_col = settings['outputcol']
    if len(df) == 0:
        df[output_col] = np.nan
    else:
        input_col = settings['inputcol']
        from nltk.util import ngrams

        n = settings['n']

        def ngrammer(row):
            return [" ".join(gram) for gram in ngrams(row, n)]

        values = df[input_col].tolist()
        grams = [ngrammer(row) for row in values]
        df[output_col] = grams

    info = [df.columns.tolist(), df.dtypes.values, [len(df)]]
    return df, info


class OneHotEncoder(ModelDDF):
    """
    Encode categorical integer features as a one-hot numeric array.

    :Example:

    >>> enc = OneHotEncoder(input_col='col_1', output_col='col_2')
    >>> ddf2 = enc.fit_transform(ddf1)
    """

    def __init__(self, input_col, output_col=None):
        """
        :param input_col: Input column name with the tokens;
        :param output_col: Output column name;
        """
        super(OneHotEncoder, self).__init__()

        if not isinstance(input_col, list):
            input_col = [input_col]

        if output_col is None:
            output_col = 'features_onehot'
        elif isinstance(output_col, list):
            raise Exception("You must insert only one name to output.")

        self.settings = dict()
        self.settings['input_col'] = input_col
        self.settings['output_col'] = output_col

        self.model = {}
        self.name = 'OneHotEncoder'

    def fit(self, data):
        """
        Fit the model.

        :param data: DDF
        :return: a trained model
        """

        df, nfrag, tmp = self._ddf_inital_setup(data)

        result_p = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result_p[f] = _one_hot_encoder(df[f], self.settings)

        categories = merge_reduce(_one_hot_encoder_merge, result_p)

        self.model['categories'] = compss_wait_on(categories)
        self.model['name'] = 'OneHotEncoder'

        return self

    def fit_transform(self, data):
        """
        Fit the model and transform.

        :param data: DDF
        :return: DDF
        """

        self.fit(data)
        ddf = self.transform(data)

        return ddf

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """
        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        df, nfrag, tmp = self._ddf_inital_setup(data)

        categories = self.model['categories']

        result = [[] for _ in range(nfrag)]
        info = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f], info[f] = _transform_one_hot(df[f], categories,
                                                    self.settings)

        uuid_key = self._ddf_add_task(task_name='transform_one_hot_encoder',
                                      status='COMPLETED', lazy=False,
                                      function={0: result},
                                      parent=[tmp.last_uuid],
                                      n_output=1, n_input=1, info=info)

        self._set_n_input(uuid_key, 0)
        return DDF(task_list=tmp.task_list, last_uuid=uuid_key)


@task(returns=1)
def _one_hot_encoder(df, settings):
    from sklearn.preprocessing import OneHotEncoder
    input_col = settings['input_col']
    X = df[input_col].values

    if len(X) > 0:
        enc = OneHotEncoder(handle_unknown='ignore')
        enc.fit(X)
        categories = enc.categories_
    else:
        categories = [np.array([]) for _ in range(len(input_col))]

    return categories


@task(returns=1)
def _one_hot_encoder_merge(x1, x2):

    x = []
    for i1, i2 in zip(x1, x2):
        t = np.unique(np.concatenate((i1, i2), axis=0))
        x.append(t)

    return x


@task(returns=2)
def _transform_one_hot(df, categories, settings):
    from sklearn.preprocessing import OneHotEncoder
    input_col = settings['input_col']
    output_col = settings['output_col']

    if len(df) == 0:
        df[output_col] = np.nan
    else:
        enc = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=np.int)
        enc._legacy_mode = False
        enc.categories_ = categories

        df[output_col] = enc.transform(df[input_col].values).tolist()

    info = [df.columns.tolist(), df.dtypes.values, [len(df)]]
    return df, info


class CountVectorizer(ModelDDF):
    """
    Converts a collection of text documents to a matrix of token counts.

    :Example:

    >>> cv = CountVectorizer(input_col='col_1', output_col='col_2').fit(ddf1)
    >>> ddf2 = cv.transform(ddf1)
    """

    def __init__(self, input_col, output_col=None, vocab_size=-1, min_tf=1.0,
                 min_df=1.0, binary=True):
        """
        :param input_col: Input column name with the tokens;
        :param output_col: Output column name;
        :param vocab_size: Maximum size of the vocabulary. If -1, no limits
         will be applied. (default, -1)
        :param min_tf: Specifies the minimum number of different documents a
         term must appear in to be included in the vocabulary. If this is an
         integer >= 1, this specifies the number of documents the term must
         appear in;  Default 1.0;
        :param min_df: Filter to ignore rare words in a document. For each
         document, terms with frequency/count less than the given threshold
         are ignored. If this is an integer >= 1, then this specifies a count
         (of times the term must appear in the document);
        :param binary: If True, all nonzero counts are set to 1.
        """
        super(CountVectorizer, self).__init__()

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['input_col'] = [input_col]
        self.settings['output_col'] = output_col
        self.settings['vocab_size'] = vocab_size
        self.settings['min_tf'] = min_tf
        self.settings['min_df'] = min_df
        self.settings['binary'] = binary

        self.model = []
        self.name = 'CountVectorizer'

    def fit(self, data):
        """
        Fit the model.

        :param data: DDF
        :return: a trained model
        """

        vocab_size = self.settings['vocab_size']
        min_tf = self.settings['min_tf']
        min_df = self.settings['min_df']

        df, nfrag, tmp = self._ddf_inital_setup(data)

        result_p = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result_p[f] = wordCount(df[f], self.settings)
        word_dic = merge_reduce(merge_wordCount, result_p)

        vocabulary = create_vocabulary(word_dic)

        if any([min_tf > 0, min_df > 0, vocab_size > 0]):
            vocabulary = filter_words(vocabulary, self.settings)

        self.model = [vocabulary]

        return self

    def fit_transform(self, data):
        """
        Fit the model and transform.

        :param data: DDF
        :return: DDF
        """

        self.fit(data)
        ddf = self.transform(data)

        return ddf

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """
        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        vocabulary = self.model[0]

        df, nfrag, tmp = self._ddf_inital_setup(data)

        result = [[] for _ in range(nfrag)]
        info = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f], info[f] = _transform_BoW(df[f], vocabulary,
                                                self.settings)

        uuid_key = self._ddf_add_task(task_name='transform_count_vectorizer',
                                      status='COMPLETED', lazy=False,
                                      function={0: result},
                                      parent=[tmp.last_uuid],
                                      n_output=1, n_input=1, info=info)

        self._set_n_input(uuid_key, 0)
        return DDF(task_list=tmp.task_list, last_uuid=uuid_key)


@task(returns=dict)
def wordCount(data, params):
    """Auxilar method to create a model."""
    wordcount = {}
    columns = params['input_col']
    # first:   Number of all occorrences with term t
    # second:  Number of diferent documents with term t
    # third:   temporary - only to idetify the last occorrence

    for i_doc, doc in enumerate(data[columns].values):
        doc = np.array(list(itertools.chain(doc))).flatten()
        for token in doc:
            if token not in wordcount:
                wordcount[token] = [1, 1, i_doc]
            else:
                wordcount[token][0] += 1
                if wordcount[token][2] != i_doc:
                    wordcount[token][1] += 1
                    wordcount[token][2] = i_doc
    return wordcount


@task(returns=1)
def merge_wordCount(dic1, dic2):
    """Merge the wordcounts."""
    for k in dic2:
        if k in dic1:
            dic1[k][0] += dic2[k][0]
            dic1[k][1] += dic2[k][1]
        else:
            dic1[k] = dic2[k]
    return dic1


@task(returns=1)
def merge_lists(list1, list2):
    """Auxiliar method."""
    list1 = list1+list2
    return list1


@local
def create_vocabulary(word_dic):
    """Create a partial mode."""
    docs_list = [[i[0], i[1][0], i[1][1]] for i in word_dic.items()]
    names = ['Word', 'TotalFrequency', 'DistinctFrequency']
    voc = pd.DataFrame(docs_list, columns=names)\
        .sort_values(by=['Word'])
    return voc


def filter_words(vocabulary, params):
    """Filter words."""
    min_df = params['min_df']
    min_tf = params['min_tf']
    size = params['vocab_size']
    if min_df > 0:
        vocabulary = vocabulary.loc[vocabulary['DistinctFrequency'] >= min_df]
    if min_tf > 0:
        vocabulary = vocabulary.loc[vocabulary['TotalFrequency'] >= min_tf]
    if size > 0:
        vocabulary = vocabulary.sort_values(['TotalFrequency'],
                                            ascending=False). head(size)

    return vocabulary


@task(returns=2)
def _transform_BoW(data, vocabulary, params):
    alias = params['output_col']
    columns = params['input_col']
    binary = params['binary']
    vector = np.zeros((len(data), len(vocabulary)), dtype=np.int)

    vocabulary = vocabulary['Word'].values
    data.reset_index(drop=True, inplace=True)
    for i, point in data.iterrows():
        lines = point[columns].values
        lines = np.array(list(itertools.chain(lines))).flatten()
        for e, w in enumerate(vocabulary):
            if binary:
                if w in lines:
                    vector[i][e] = 1
                else:
                    vector[i][e] = 0
            else:
                vector[i][e] = (lines == w).sum()

    data[alias] = vector.tolist()

    info = [data.columns.tolist(), data.dtypes.values, [len(data)]]
    return data, info


class TfidfVectorizer(ModelDDF):
    """
    Term frequency-inverse document frequency (TF-IDF) is a numerical
    statistic transformation that is intended to reflect how important a word
    is to a document in a collection or corpus.

    :Example:

    >>> tfidf = TfidfVectorizer(input_col='col_0', output_col='col_1').fit(ddf1)
    >>> ddf2 = tfidf.transform(ddf1)
    """

    def __init__(self, input_col, output_col=None, vocab_size=-1, min_tf=1.0,
                 min_df=1.0):
        """
        :param input_col: Input column name with the tokens;
        :param output_col: Output column name;
        :param vocab_size: Maximum size of the vocabulary. If -1, no limits
         will be applied. (default, -1)
        :param min_tf: Specifies the minimum number of different documents a
         term must appear in to be included in the vocabulary. If this is an
         integer >= 1, this specifies the number of documents the term must
         appear in;  Default 1.0;
        :param min_df: Filter to ignore rare words in a document. For each
         document, terms with frequency/count less than the given threshold
         are ignored. If this is an integer >= 1, then this specifies a count
         (of times the term must appear in the document);
        """
        super(TfidfVectorizer, self).__init__()

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['input_col'] = [input_col]
        self.settings['output_col'] = output_col
        self.settings['vocab_size'] = vocab_size
        self.settings['min_tf'] = min_tf
        self.settings['min_df'] = min_df

        self.model = []
        self.name = 'TfidfVectorizer'

    def fit(self, data):
        """
        Fit the model.

        :param data: DDF
        :return: trained model
        """

        vocab_size = self.settings['vocab_size']
        min_tf = self.settings['min_tf']
        min_df = self.settings['min_df']

        df, nfrag, tmp = self._ddf_inital_setup(data)

        result_p = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result_p[f] = wordCount(df[f], self.settings)
        word_dic = merge_reduce(merge_wordCount, result_p)
        vocabulary = create_vocabulary(word_dic)

        if any([min_tf > 0, min_df > 0, vocab_size > 0]):
            vocabulary = filter_words(vocabulary, self.settings)

        self.model = [vocabulary]

        return self

    def fit_transform(self, data):
        """
        Fit the model and transform.

        :param data: DDF
        :return: DDF
        """

        self.fit(data)
        ddf = self.transform(data)

        return ddf

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")
        vocabulary = self.model[0]

        df, nfrag, tmp = self._ddf_inital_setup(data)

        counts = [count_records(df[f]) for f in range(nfrag)]
        count = merge_reduce(mergeCount, counts)

        result = [[] for _ in range(nfrag)]
        info = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f], info[f] = \
                construct_tf_idf(df[f], vocabulary, self.settings, count)

        uuid_key = self._ddf_add_task(task_name='task_transform_tfidf',
                                      status='COMPLETED', lazy=False,
                                      function={0: result},
                                      parent=[tmp.last_uuid],
                                      n_output=1, n_input=1, info=info)

        self._set_n_input(uuid_key, 0)
        return DDF(task_list=tmp.task_list, last_uuid=uuid_key)


@task(returns=list)
def count_records(data):
    """Count the partial number of records in each fragment."""
    return len(data)


@task(returns=list)
def mergeCount(data1, data2):
    """Auxiliar method to merge the lengths."""
    return data1 + data2


@task(returns=2)
def construct_tf_idf(data, vocabulary, params, num_doc):
    """Construct the tf-idf feature.

    TF(t)  = (Number of times term t appears in a document)
                    / (Total number of terms in the document).
    IDF(t) = log( Total number of documents /
                    Number of documents with term t in it).
    Source: http://www.tfidf.com/
    """

    alias = params['output_col']
    columns = params['input_col']
    vector = np.zeros((data.shape[0], vocabulary.shape[0]), dtype=np.float)
    vocab = vocabulary['Word'].values
    data.reset_index(drop=True, inplace=True)

    for i, point in data.iterrows():
        lines = point[columns].values
        lines = np.array(list(itertools.chain(lines))).flatten()
        for w, token in enumerate(vocab):
            if token in lines:
                # TF = (Number of times term t appears in the document) /
                #        (Total number of terms in the document).
                nTimesTermT = np.count_nonzero(lines == token)
                total = len(lines)
                if total > 0:
                    tf = float(nTimesTermT) / total
                else:
                    tf = 0

                # IDF = log_e(Total number of documents /
                #            Number of documents with term t in it).
                nDocsWithTermT = vocabulary.\
                    loc[vocabulary['Word'] == token, 'DistinctFrequency'].\
                    item()
                idf = np.log(float(num_doc) / nDocsWithTermT)

                vector[i][w] = tf*idf

    data[alias] = vector.tolist()

    info = [data.columns.tolist(), data.dtypes.values, [len(data)]]
    return data, info


class MinMaxScaler(ModelDDF):
    """
    MinMaxScaler transforms a dataset of features rows, rescaling
    each feature to a specific range (often [0, 1])

    The rescaled value for a feature E is calculated as:

    Rescaled(ei) = (ei − Emin)∗(max − min)/(Emax − Emin) + min

    For the case Emax == Emin,  Rescaled(ei) = 0.5∗(max + min)

    :Example:

    >>> scaler = MinMaxScaler(input_col='features',
    >>>                       output_col='output').fit(ddf1)
    >>> ddf2 = scaler.transform(ddf1)
    """

    def __init__(self, input_col, output_col, feature_range=(0, 1)):
        """
        :param input_col: Column with the features;
        :param output_col: Output column;
        :param feature_range: A tuple with the range, default is (0,1).
        """
        super(MinMaxScaler, self).__init__()

        if not output_col:
            output_col = input_col

        if not isinstance(feature_range, tuple) or \
                feature_range[0] >= feature_range[1]:
            raise Exception("You must inform a valid `feature_range`.")

        self.settings = dict()
        self.settings['input_col'] = [input_col]
        self.settings['output_col'] = [output_col]
        self.settings['feature_range'] = feature_range

        self.model = []
        self.name = 'MinMaxScaler'

    def fit(self, data):
        """
        Fit the model.

        :param data: DDF
        :return: trained model
        """

        df, nfrag, tmp = self._ddf_inital_setup(data)

        columns = self.settings['input_col']
        # generate a list of the min and the max element to each subset.
        minmax_partial = \
            [_agg_maxmin(df[f], columns) for f in range(nfrag)]

        # merge them into only one list
        minmax = merge_reduce(_merge_maxmin, minmax_partial)
        minmax = compss_wait_on(minmax)

        self.model = [minmax]
        return self

    def fit_transform(self, data):
        """
        Fit the model and transform.

        :param data: DDF
        :return: DDF
        """

        self.fit(data)
        ddf = self.transform(data)

        return ddf

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        df, nfrag, tmp = self._ddf_inital_setup(data)

        result = [[] for _ in range(nfrag)]
        info = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f], info[f] = _minmax_scaler(df[f], self.settings,
                                                self.model[0])

        uuid_key = self._ddf_add_task(task_name='task_transform_minmax_scaler',
                                      status='COMPLETED', lazy=False,
                                      function={0: result},
                                      parent=[tmp.last_uuid],
                                      n_output=1, n_input=1, info=info)

        self._set_n_input(uuid_key, 0)
        return DDF(task_list=tmp.task_list, last_uuid=uuid_key)


@task(returns=list)
def _agg_maxmin(df, columns):
    """Generate a list of min and max values, excluding NaN values."""
    min_max_p = []
    if len(df) > 0:
        for col in columns:
            p = [np.min(df[col].values, axis=0), np.max(df[col].values, axis=0)]
            min_max_p.append(p)
    return min_max_p


@task(returns=list)
def _merge_maxmin(minmax1, minmax2):
    """Merge min and max values."""
    minmax = []
    if len(minmax1) > 0 and len(minmax2) > 0:
        for feature in zip(minmax1, minmax2):
            di, dj = feature
            minimum = di[0] if di[0] < dj[0] else dj[0]
            maximum = di[1] if di[1] > dj[1] else dj[1]
            minmax.append([minimum, maximum])
    elif len(minmax1) == 0:
        minmax = minmax2
    else:
        minmax = minmax1
    return minmax


@task(returns=2)
def _minmax_scaler(data, settings, minmax):
    """Normalize by min max mode."""
    features = settings['input_col']
    alias = settings.get('output_col', [])
    min_r, max_r = settings.get('feature_range', (0, 1))

    if len(alias) != len(features):
        alias = features

    def calculation(xs, minimum, maximum, min_r, max_r):
        features = []
        diff_r = float(max_r - min_r)
        for xi, mi, ma in zip(xs, minimum, maximum):
            if ma == mi:
                v = 0.5 * (max_r + min_r)
            else:
                v = (float(xi - mi) * (diff_r / (ma - mi))) + min_r
            features.append(v)
        return features

    for i, (alias, col) in enumerate(zip(alias, features)):
        minimum, maximum = minmax[i]
        data[alias] = data[col].apply(
                lambda xs: calculation(xs, minimum, maximum, min_r, max_r))

    info = [data.columns.tolist(), data.dtypes.values, [len(data)]]
    return data, info


class MaxAbsScaler(ModelDDF):
    """
    MaxAbsScaler transforms a dataset of features rows,
    rescaling each feature to range [-1, 1] by dividing through
    the maximum absolute value in each feature.

    This estimator scales and translates each feature individually
    such that the maximal absolute value of each feature in the
    training set will be 1.0. It does not shift/center the data,
    and thus does not destroy any sparsity.

    :Example:

    >>> scaler = MaxAbsScaler(input_col='features',
    >>>                       output_col='features_norm').fit(ddf1)
    >>> ddf2 = scaler.transform(ddf1)
    """

    def __init__(self, input_col, output_col):
        """
        :param input_col: Column with the features;
        :param output_col: Output column;
        """
        super(MaxAbsScaler, self).__init__()

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['input_col'] = [input_col]
        self.settings['output_col'] = [output_col]

        self.model = []
        self.name = 'MaxAbsScaler'

    def fit(self, data):
        """
        Fit the model.

        :param data: DDF
        :return: trained model
        """

        df, nfrag, tmp = self._ddf_inital_setup(data)

        columns = self.settings['input_col']
        # generate a list of the min and the max element to each subset.
        minmax_partial = \
            [_agg_maxabs(df[f], columns) for f in range(nfrag)]

        # merge them into only one list
        minmax = merge_reduce(_merge_maxabs, minmax_partial)
        minmax = compss_wait_on(minmax)

        self.model = [minmax]
        return self

    def fit_transform(self, data):
        """
        Fit the model and transform.

        :param data: DDF
        :return: DDF
        """

        self.fit(data)
        ddf = self.transform(data)

        return ddf

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        df, nfrag, tmp = self._ddf_inital_setup(data)

        result = [[] for _ in range(nfrag)]
        info = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f], info[f] = _maxabs_scaler(df[f], self.model[0],
                                                self.settings)

        uuid_key = self._ddf_add_task(task_name='task_transform_maxabs_scaler',
                                      status='COMPLETED', lazy=False,
                                      function={0: result},
                                      parent=[tmp.last_uuid],
                                      n_output=1, n_input=1, info=info)

        self._set_n_input(uuid_key, 0)
        return DDF(task_list=tmp.task_list, last_uuid=uuid_key)


@task(returns=list)
def _agg_maxabs(df, columns):
    """Generate a list of min and max values, excluding NaN values."""
    min_max_p = []

    if len(df) > 0:
        for col in columns:
            p = [np.min(df[col].values, axis=0), np.max(df[col].values, axis=0)]
            min_max_p.append(p)

    return min_max_p


@task(returns=list)
def _merge_maxabs(minmax1, minmax2):
    """Merge max abs values."""
    maxabs = []
    if len(minmax1) > 0 and len(minmax2) > 0:
        for feature in zip(minmax1, minmax2):
            d_esq, d_dir = feature

            minimum = [min([di, dj]) for di, dj in zip(d_esq[0], d_dir[0])]
            maximum = [max([di, dj]) for di, dj in zip(d_esq[1], d_dir[1])]
            maxabs.append([minimum, maximum])
    elif len(minmax1) > len(minmax2):
        maxabs = minmax1
    else:
        maxabs = minmax2

    return maxabs


@task(returns=2)
def _maxabs_scaler(data, minmax, settings):
    """Normalize by range mode."""
    features = settings['input_col']
    alias = settings.get('output_col', [])

    if len(alias) != len(features):
        alias = features

    def calculation(xs, minimum, maximum):
        features = []
        for xi, mi, ma in zip(xs, minimum, maximum):
            ma = abs(ma)
            mi = abs(mi)
            maxabs = ma if ma > mi else mi
            v = float(xi) / maxabs
            features.append(v)
        return features

    for i, (alias, col) in enumerate(zip(alias, features)):
        minimum, maximum = minmax[i]
        data[alias] = data[col].apply(
                lambda xs: calculation(xs, minimum, maximum))

    info = [data.columns.tolist(), data.dtypes.values, [len(data)]]
    return data, info


class StandardScaler(ModelDDF):
    """
    The standard score of a sample x is calculated as:

        z = (x - u) / s

    where u is the mean of the training samples or zero if
    with_mean=False, and s is the standard deviation of the
    training samples or one if with_std=False.

    :Example:

    >>> scaler = StandardScaler(input_col='features',
    >>>                         output_col='norm').fit(ddf1)
    >>> ddf2 = scaler.transform(ddf1)
    """

    def __init__(self, input_col, output_col=None,
                 with_mean=True, with_std=True):
        """
        :param input_col: Column with the features;
        :param output_col: Output column;
        :param with_mean: True to use the mean (default is True);
        :param with_std: True to use standard deviation of the
         training samples (default is True).
        """
        super(StandardScaler, self).__init__()

        if not output_col:
            output_col = input_col

        self.settings = dict()
        self.settings['input_col'] = [input_col]
        self.settings['output_col'] = [output_col]
        self.settings['with_mean'] = with_mean
        self.settings['with_std'] = with_std

        self.model = []
        self.name = 'StandardScaler'

    def fit(self, data):
        """
        Fit the model.

        :param data: DDF
        :return: trained model
        """

        df, nfrag, tmp = self._ddf_inital_setup(data)

        features = self.settings['input_col']

        # compute the sum of each subset column
        sums = [_agg_sum(df[f], features) for f in range(nfrag)]
        # merge then to compute a mean
        mean = merge_reduce(_merge_sum, sums)

        # using this mean, compute the variance of each subset column
        sse = [_agg_sse(df[f], features, mean) for f in range(nfrag)]
        sse = merge_reduce(_merge_sse, sse)

        mean = compss_wait_on(mean)
        sse = compss_wait_on(sse)
        self.model = [[mean, sse]]

        return self

    def fit_transform(self, data):
        """
        Fit the model and transform.

        :param data: DDF
        :return: DDF
        """

        self.fit(data)
        ddf = self.transform(data)

        return ddf

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        df, nfrag, tmp = self._ddf_inital_setup(data)

        mean, sse = self.model[0]
        result = [[] for _ in range(nfrag)]
        info = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f], info[f] = _stardard_scaler(df[f], self.settings,
                                                  mean, sse)

        uuid_key = self._ddf_add_task(task_name='transform_standard_scaler',
                                      status='COMPLETED', lazy=False,
                                      function={0: result},
                                      parent=[tmp.last_uuid],
                                      n_output=1, n_input=1, info=info)

        self._set_n_input(uuid_key, 0)
        return DDF(task_list=tmp.task_list, last_uuid=uuid_key)


@task(returns=1)
def _agg_sum(df, features):
    """Pre-compute some values."""
    sum_partial = []
    for feature in features:
        sum_p = [np.nansum(df[feature].values.tolist(), axis=0),
                 len(df[feature])]
        sum_partial.append(sum_p)
    return sum_partial


@task(returns=1, priority=True)
def _merge_sum(sum1, sum2):
    """Merge pre-computation."""
    sum_count = []
    for f_i, f_j in zip(sum1, sum2):
        count = f_i[1] + f_j[1]
        sums = []
        for di, dj in zip(f_i[0], f_j[0]):
            sum_f = di + dj
            sums.append(sum_f)
        sum_count.append([sums, count])

    return sum_count


@task(returns=1)
def _agg_sse(df, features, sum_count):
    """Perform a partial SSE calculation."""
    sum_sse = []

    def computation_sse(xs, means):
        sse = []
        for xi, mi in zip(xs, means):
            sse.append((xi - mi) ** 2)
        return sse

    for sum_f, col in zip(sum_count, features):
        size = sum_f[1]
        sums = sum_f[0]
        means = [float(x) / size for x in sums]

        sum_sse.append(
                np.nansum(df[col].apply(lambda xs: computation_sse(xs, means))
                          .values.tolist(), axis=0))

    return sum_sse


@task(returns=list, priority=True)
def _merge_sse(sum1, sum2):
    """Merge the partial SSE."""
    sum_count = []
    for di, dj in zip(sum1, sum2):
        sum_count.append(di + dj)
    return sum_count


@task(returns=2)
def _stardard_scaler(data, settings, mean, sse):
    """Normalize by Standard mode."""
    features = settings['input_col']
    alias = settings['output_col']
    with_mean = settings['with_mean']
    with_std = settings['with_std']

    if len(alias) != len(features):
        alias = features

    from sklearn.preprocessing import StandardScaler
    for i, (alias, col) in enumerate(zip(alias, features)):
        size = mean[i][1]

        var_ = [float(sse_p) / size for sse_p in sse[i]]
        mean_ = [float(x) / size for x in mean[i][0]]

        scaler = StandardScaler()
        scaler.mean_ = mean_ if with_mean else None
        scaler.scale_ = np.sqrt(var_) if with_std else None
        scaler.var_ = var_ if with_std else None
        scaler.n_samples_seen_ = size

        values = data[col].tolist()
        data[alias] = scaler.transform(values).tolist()

    info = [data.columns.tolist(), data.dtypes.values, [len(data)]]
    return data, info


class StringIndexer(ModelDDF):
    """
    StringIndexer indexes a feature by encoding a string column as a
    column containing indexes.

    :Example:

    >>> model = StringIndexer(input_col='category').fit(ddf1)
    >>> ddf2 = model.transform(ddf1)
    """

    def __init__(self, input_col, output_col=None):
        """

        :param input_col: Input string column;
        :param output_col:  Output indexes column.
        """
        super(StringIndexer, self).__init__()

        if not output_col:
            output_col = "{}_indexed".format(input_col)

        self.settings = dict()
        self.settings['input_col'] = input_col
        self.settings['output_col'] = output_col

        self.model = {}
        self.name = 'StringIndexer'

    def fit(self, data):
        """
        Fit the model.

        :param data: DDF
        :return: a trained model
        """

        in_col = self.settings['input_col']

        df, nfrag, tmp = self._ddf_inital_setup(data)

        mapper = [get_indexes(df[f], in_col) for f in range(nfrag)]
        mapper = merge_reduce(merge_mapper, mapper)

        self.model['model'] = compss_wait_on(mapper)
        return self

    def fit_transform(self, data):
        """
        Fit the model and transform.

        :param data: DDF
        :return: DDF
        """

        self.fit(data)
        ddf = self.transform(data)

        return ddf

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        in_col = self.settings['input_col']
        out_col = self.settings['output_col']

        df, nfrag, tmp = self._ddf_inital_setup(data)

        result = [[] for _ in range(nfrag)]
        info = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f], info[f] = _string_to_indexer(df[f], in_col, out_col,
                                                    self.model['model'])

        uuid_key = self._ddf_add_task(task_name='transform_string_indexer',
                                      status='COMPLETED', lazy=False,
                                      function={0: result},
                                      parent=[tmp.last_uuid],
                                      n_output=1, n_input=1, info=info)

        self._set_n_input(uuid_key, 0)
        return DDF(task_list=tmp.task_list, last_uuid=uuid_key)


@task(returns=1)
def get_indexes(data, in_col):
    """Create partial model to convert string to index."""
    return data[in_col].dropna().unique()


@task(returns=1)
def merge_mapper(data1, data2):
    """Merge partial models into one."""
    data1 = np.concatenate((data1, data2), axis=0)
    return np.unique(data1)


@task(returns=2)
def _string_to_indexer(data, in_col, out_col, mapper):
    """Convert string to index based in the model."""
    news = [i for i in range(len(mapper))]
    mapper = mapper.tolist()
    data[out_col] = data[in_col].replace(to_replace=mapper, value=news)

    info = [data.columns.tolist(), data.dtypes.values, [len(data)]]
    return data, info


class IndexToString(ModelDDF):
    """
    Symmetrically to StringIndexer, IndexToString maps a column of
    label indices back to a column containing the original labels as strings.

    :Example:

    >>> ddf2 = IndexToString(input_col='category_indexed',
    >>>                      model=model).transform(ddf1)
    """

    def __init__(self, input_col, model, output_col=None):
        """
        :param input_col: Input column name;
        :param model: Model generated by StringIndexer;
        :param output_col: Output column name.
        """
        super(IndexToString, self).__init__()

        if not output_col:
            output_col = "{}_converted".format(input_col)

        self.settings = dict()
        self.settings['input_col'] = input_col
        self.settings['output_col'] = output_col

        self.model = model.model
        self.name = 'IndexToString'

    def transform(self, data):

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        input_col = self.settings['input_col']
        output_col = self.settings['output_col']

        df, nfrag, tmp = self._ddf_inital_setup(data)

        result = [[] for _ in range(nfrag)]
        info = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f], info[f] = _index_to_string(df[f], input_col,
                                                  output_col,
                                                  self.model['model'])

        uuid_key = self._ddf_add_task(task_name='transform_indextostring',
                                      status='COMPLETED', lazy=False,
                                      function={0: result},
                                      parent=[tmp.last_uuid],
                                      n_output=1, n_input=1, info=info)

        self._set_n_input(uuid_key, 0)
        return DDF(task_list=tmp.task_list, last_uuid=uuid_key)


@task(returns=2)
def _index_to_string(data, input_col, output_col, mapper):
    """Convert index to string based in the model."""
    news = [i for i in range(len(mapper))]
    mapper = mapper.tolist()
    data[output_col] = data[input_col].replace(to_replace=news, value=mapper)

    info = [data.columns.tolist(), data.dtypes.values, [len(data)]]
    return data, info


class PCA(ModelDDF):
    """
    Principal component analysis (PCA) is a statistical method to find
    a rotation such that the first coordinate has the largest variance
    possible, and each succeeding coordinate in turn has the largest
    variance possible. The columns of the rotation matrix are called
    principal components. PCA is used widely in dimensionality reduction.

    :Example:

    >>> pca = PCA(input_col='features', output_col='features_pca',
    >>>           n_components=2).fit(ddf1)
    >>> ddf2 = pca.transform(ddf1)
    """

    def __init__(self, input_col, n_components, output_col=None):
        """
        :param input_col: Input feature column;
        :param n_components: Number of output components;
        :param output_col: Output feature column (default, *'prediction_PCA'*).
        """
        super(PCA, self).__init__()

        if not output_col:
            output_col = 'prediction_PCA'

        self.settings = dict()
        self.settings['input_col'] = input_col
        self.settings['output_col'] = output_col
        self.settings['n_components'] = n_components

        self.name = 'PCA'
        self.var_exp = self.cum_var_exp = \
            self.eig_vals = self.eig_vecs = self.matrix = 0

    def fit(self, data):
        """
        :param data: DDF
        :return: trained model
        """

        df, nfrag, tmp = self._ddf_inital_setup(data)

        n_components = self.settings.get('n_components')
        cols = self.settings.get('input_col')

        partial_count = [pca_count(df[f], cols) for f in range(nfrag)]
        merged_count = merge_reduce(pca_merge_count, partial_count)

        for f in range(nfrag):
            partial_count[f] = partial_multiply(df[f], cols, merged_count)

        merged_cov = merge_reduce(pca_cov_merger, partial_count)

        info = pca_eigen_decomposition(merged_cov, n_components)
        self.var_exp, self.eig_vals, self.eig_vecs, self.matrix = info

        self.cum_var_exp = np.cumsum(self.var_exp)

        self.model = dict()
        self.model['algorithm'] = self.name
        # cumulative explained variance
        self.model['cum_var_exp'] = self.cum_var_exp
        self.model['eig_vals'] = self.eig_vals
        self.model['eig_vecs'] = self.eig_vecs
        self.model['model'] = self.matrix

        return self

    def fit_transform(self, data):
        """
        Fit the model and transform.

        :param data: DDF
        :return: DDF
        """

        self.fit(data)
        ddf = self.transform(data)

        return ddf

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """
        df, nfrag, tmp = self._ddf_inital_setup(data)

        if len(self.model) == 0:
            raise Exception("Model is not fitted.")

        model = self.model['model']
        features_col = self.settings['input_col']
        pred_col = self.settings['output_col']

        result = [[] for _ in range(nfrag)]
        info = [[] for _ in range(nfrag)]
        for f in range(nfrag):
            result[f], info[f] = _pca_transform(df[f], features_col,
                                                pred_col, model)

        uuid_key = self._ddf_add_task(task_name='transform_pca',
                                      status='COMPLETED', lazy=False,
                                      function={0: result},
                                      parent=[tmp.last_uuid],
                                      n_output=1, n_input=1, info=info)

        # a ml.transform will always have cache() before
        self._set_n_input(uuid_key, 0)
        return DDF(task_list=tmp.task_list, last_uuid=uuid_key)


@task(returns=1)
def pca_count(data, cols):
    """Partial count."""
    partial_size = len(data)
    partial_sum = 0
    if partial_size > 0:
        data = data[cols].values
        partial_sum = reduce(lambda l1, l2: np.add(l1, l2), data)
    return [partial_size, partial_sum]


@task(returns=1)
def pca_merge_count(count1, count2):
    """Merge partial counts."""
    partial_size = count1[0] + count2[0]
    partial_sum = np.add(count1[1], count2[1])
    return [partial_size, partial_sum]


@task(returns=1)
def partial_multiply(data, col, info):
    """Perform partial calculation."""
    cov_mat = 0
    total_size = info[0]

    if len(data) > 0:
        mean_vec = np.array(map(lambda x: float(x) / total_size, info[1]))

        x_std = data[col].values
        x_std = np.array(x_std.tolist())

        first_part = x_std - mean_vec
        cov_mat = first_part.T.dot(first_part)

    return [cov_mat, total_size]


@task(returns=1)
def pca_cov_merger(info1, info2):
    """Merge covariance."""
    cov1, total_size = info1
    cov2, _ = info2

    return [np.add(cov1, cov2), total_size]

@local
def pca_eigen_decomposition(info, n_components):
    """Generate an eigen decomposition."""

    cov_mat, total_size = info
    dim = len(cov_mat)
    n_components = min([n_components, dim])
    cov_mat = cov_mat / (total_size-1)
    eig_vals, eig_vecs = np.linalg.eig(cov_mat)
    eig_vals = np.abs(eig_vals)

    total_values = sum(eig_vals)
    var_exp = [(float(i) / total_values)*100 for i in eig_vals]

    # Sort the eigenvalue and vecs tuples from high to low
    idxs = eig_vals.argsort()[::-1]
    eig_vals = eig_vals[idxs]
    eig_vecs = eig_vecs[idxs]

    matrix_w = eig_vecs[:, :n_components]

    return var_exp, eig_vals, eig_vecs, matrix_w


@task(returns=2)
def _pca_transform(data, features, pred_col, matrix_w):
    """Reduce the dimensionality based in the created model."""
    tmp = []
    if len(data) > 0:
        array = np.array(data[features].values.tolist())
        tmp = array.dot(matrix_w).tolist()

    data[pred_col] = tmp

    info = [data.columns.tolist(), data.dtypes.values, [len(data)]]
    return data, info


class PolynomialExpansion(object):

    """
    Perform feature expansion in a polynomial space. As said in wikipedia of
    Polynomial Expansion, “In mathematics, an expansion of a product of sums
    expresses it as a sum of products by using the fact that multiplication
    distributes over addition”.

    For example, if an input sample is two dimensional and of the form [a, b],
    the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]
    """

    def __init__(self, input_col, degree=2, output_col=None,
                 interaction_only=False):
        """
       :param input_col: List of columns;
       :param output_col: Output column name.
       :param degree: The degree of the polynomial features. Default = 2.
       :param interaction_only: If true, only interaction features are
        produced: features that are products of at most degree distinct input
        features. Default = False
       """

        if not output_col:
            output_col = 'features'

        self.settings = dict()
        self.settings['inputcol'] = input_col
        self.settings['outputcol'] = output_col
        self.settings['degree'] = int(degree)
        self.settings['interaction_only'] = interaction_only

    def transform(self, data):
        """
        :param data: DDF
        :return: DDF
        """

        def task_poly_expansion(df, params):
            return _poly_expansion(df, params)

        uuid_key = data._ddf_add_task(task_name='poly_expansion',
                                      status='WAIT', lazy=True,
                                      function=[task_poly_expansion,
                                                self.settings],
                                      parent=[data.last_uuid],
                                      n_output=1, n_input=1)

        data._set_n_input(uuid_key, data.settings['input'])
        return DDF(task_list=data.task_list, last_uuid=uuid_key)


def _poly_expansion(df, params):

    output_col = params['outputcol']

    if len(df) == 0:
        df[output_col] = np.nan
    else:
        input_col = params['inputcol']
        interaction_only = params['interaction_only']
        degree = params['degree']

        from sklearn.preprocessing import PolynomialFeatures
        values = np.array(df[input_col].tolist())
        values = PolynomialFeatures(degree=degree,
                                    interaction_only=interaction_only)\
            .fit_transform(values)
        df[output_col] = values.tolist()

    info = [df.columns.tolist(), df.dtypes.values, [len(df)]]
    return df, info
